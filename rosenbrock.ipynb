{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "564 HW1 code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tpw3VKf9Os"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "from numpy.linalg import inv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btAlm3kaTJN"
      },
      "source": [
        "# The rosenbrock function\r\n",
        "\r\n",
        "\r\n",
        "The Rosenbrock fucntion is a non-convex function,\r\n",
        "\r\n",
        "It is also known as Rosenbrock's valley or Rosenbrock's banana function.\r\n",
        "\r\n",
        "The global minimum is inside a long, narrow, parabolic shaped flat valley. To find the valley is trivial. To converge to the global minimum, however, is difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9WmNCPhPH-r"
      },
      "source": [
        "# Define GradientDescent on Rosenbrock function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLTRGNIugAwz"
      },
      "source": [
        "def GradientDescent_Rosenbrock(x_0,learning_rate,iteration):\n",
        "  \n",
        "  ''' define steepest descent function, inputs are initial guess vector\n",
        "      ,learning_rate alpha, and how many iteration\n",
        "   '''\n",
        "  x = x_0\n",
        "  for i in range(iteration):\n",
        "    a = x[0,0]\n",
        "    b = x[1,0]\n",
        "    grad_f = np.array([[400*(a**3) - 400*a*b + 2*a - 2],[200*b-200*(a**2)]]) #define the gradient of rosenbrock function\n",
        "    x = x - learning_rate*grad_f #algrithm for gradientDescent\n",
        "\n",
        "    print('function valuve at x is :',100*(x[1,0]-x[0,0]**2)**2 + (1-(x[0,0]))**2)\n",
        "\n",
        "  \n",
        "  return "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg0QCU_KvKtk"
      },
      "source": [
        "# Steepest descent method\n",
        "\n",
        "for Steepest descent iterations, we set the alpha to 1, because of the rosenbrock function, the solution is in a narrow flat bottomed vally, steepest descent method easily will blow up the result, as only take 4 iteration to blow up the float number tolerance in python "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH9g8i2Xu77O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd28afd8-81a3-4585-cbc9-e367633ced77"
      },
      "source": [
        "GradientDescent_Rosenbrock(x_0 = np.zeros((2,1)),learning_rate = 1,iteration =5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "function valuve at x is : 1601.0\n",
            "function valuve at x is : 1.04841216742464e+16\n",
            "function valuve at x is : 2.950556825554033e+54\n",
            "function valuve at x is : 6.575850257231013e+169\n",
            "function valuve at x is : inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in double_scalars\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipeKVkYvvjtE"
      },
      "source": [
        "# Gradient descent with very small learning rate\n",
        "\n",
        "\n",
        "here if we use Gradient descent with a very small learning rate such as 0.001, we might just get lucky after long iteration without blowing up and reach the min [1,1],after 10000 iteration we get close enough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Xx5aXOmZS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44cad622-0384-4c11-8e98-002601491f11"
      },
      "source": [
        "def GradientDescent_Rosenbrock_2(x_0,learning_rate,iteration):\n",
        "  \n",
        "  ''' define steepest descent function, inputs are initial guess vector\n",
        "      ,learning_rate alpha, and how many iteration\n",
        "   '''\n",
        "  x = x_0\n",
        "  for i in range(iteration):\n",
        "    a = x[0,0]\n",
        "    b = x[1,0]\n",
        "    grad_f = np.array([[400*(a**3) - 400*a*b + 2*a - 2],[200*b-200*(a**2)]]) #define the gradient of rosenbrock function\n",
        "    x = x - learning_rate*grad_f #algrithm for gradientDescent\n",
        "\n",
        "\n",
        "  print('function valuve at x is :',100*(x[1,0]-x[0,0]**2)**2 + (1-(x[0,0]))**2)\n",
        "\n",
        "\n",
        "  return x\n",
        "\n",
        "print(GradientDescent_Rosenbrock_2(x_0 = np.zeros((2,1)),learning_rate = 0.001953125,iteration =10000))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "function valuve at x is : 1.5165349423304142e-08\n",
            "[[0.99987695]\n",
            " [0.99975342]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-M3WTRcwawG"
      },
      "source": [
        "# Newton's method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4MaRAQ6w1hw"
      },
      "source": [
        "from numpy.linalg import inv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNC8oEXywD30"
      },
      "source": [
        "def Newtown_iteration(iteration):\n",
        "  ''' Newton's interation method, step size is 1 and starting from all zero vectors'''\n",
        "\n",
        "  x = np.zeros((2,1)) #x_0 is all zero\n",
        "  for i in range(iteration):\n",
        "    a = x[0,0]\n",
        "    b= x[1,0]\n",
        "    hessian = np.array([[-400*(b-a**2)+800*a**2+2,-400*a],[-400*a,200]])\n",
        "    hessian_inv = inv(hessian)\n",
        "    grad_f = np.array([[400*(a**3) - 400*a*b + 2*a - 2],[200*b-200*(a**2)]])\n",
        "    x = x - hessian_inv @ grad_f\n",
        "    print('funtion value at x is :',100*(x[1,0]-x[0,0]**2)**2 + (1-(x[0,0]))**2)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlGPi_dj5bMY"
      },
      "source": [
        "# Newton's method result\n",
        "\n",
        "note that it is very quick to reach target, but ,\n",
        "\n",
        "\n",
        "\n",
        "Large matrix the Hessian will be much harder to compute\n",
        " \n",
        "\n",
        "Also numerically inverting a small matrix in our exmaple is stable, but in real life to invert nxn large matrix with high condition number we need to use itertive method such as gauss seidel or Pre condition conjugate gradient just to find the Hessian inverse, the whole process will be slower than say Stochastic gradient descent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3BbmdbMxaZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd91d798-d12f-4713-8bb1-e76c3f6f0a9d"
      },
      "source": [
        "Newtown_iteration(iteration = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "funtion value at x is : 100.0\n",
            "funtion value at x is : 4.930380657631324e-30\n",
            "funtion value at x is : 1.232595164407831e-30\n",
            "funtion value at x is : 0.0\n",
            "funtion value at x is : 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkMgG-sxIrE5"
      },
      "source": [
        "# Using tensorflow build in fuctions\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QRxz0voEPqR"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53x6Kl2PCpW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8020424-67f7-480e-aa41-4da18c2e7b22"
      },
      "source": [
        "x = tf.Variable(-1.2,dtype=tf.float32)\n",
        "y = tf.Variable(1,dtype= tf.float32)\n",
        "f = 100*(y-x**2)**2 +(1-x)**2\n",
        "train = tf.train.GradientDescentOptimizer(0.002).minimize(f)\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "session = tf.Session()\n",
        "session.run(init)\n",
        "print(session.run(x),session.run(y))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "-1.2 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSM3JgiGHKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b13e325-5b2e-4c6c-adfc-d77280c6af5c"
      },
      "source": [
        "for i in range(10000):\n",
        "\n",
        "  session.run(train)\n",
        "\n",
        "print('x:',session.run(x),'\\n'\n",
        "      'y:',session.run(y),'\\n'\n",
        "      'function value:',session.run(f))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: 0.99986595 \n",
            "y: 0.9997296 \n",
            "function value: 1.8509997e-08\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}